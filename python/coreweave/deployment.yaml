# CoreWeave Kubernetes Deployment
# Babylon RL Training Infrastructure

apiVersion: v1
kind: Namespace
metadata:
  name: babylon-rl
---
# ConfigMap for environment variables
apiVersion: v1
kind: ConfigMap
metadata:
  name: babylon-rl-config
  namespace: babylon-rl
data:
  TRAINING_ENV: "coreweave"
  MIN_AGENTS_PER_WINDOW: "5"
  TRAINING_FREQUENCY_WINDOWS: "1"
  MODEL_NAME: "Qwen/Qwen2.5-7B-Instruct"
  BATCH_SIZE: "16"
  GRADIENT_ACCUMULATION_STEPS: "4"
  ITERATIONS_PER_WINDOW: "10"
  GPU_TYPE: "A100"
  GPU_COUNT: "4"
  LOG_LEVEL: "INFO"
  DEPLOYMENT_STRATEGY: "blue-green"
---
# Secret for sensitive data
apiVersion: v1
kind: Secret
metadata:
  name: babylon-rl-secrets
  namespace: babylon-rl
type: Opaque
stringData:
  DATABASE_URL: "" # Fill in via kubectl or CI/CD
  OPENPIPE_API_KEY: "" # Fill in via kubectl or CI/CD
  WANDB_API_KEY: "" # Fill in via kubectl or CI/CD
  AWS_ACCESS_KEY_ID: "" # Fill in via kubectl or CI/CD
  AWS_SECRET_ACCESS_KEY: "" # Fill in via kubectl or CI/CD
---
# PersistentVolumeClaim for checkpoints
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: babylon-rl-checkpoints
  namespace: babylon-rl
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Gi
  storageClassName: ceph-filesystem
---
# PersistentVolumeClaim for logs
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: babylon-rl-logs
  namespace: babylon-rl
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Gi
  storageClassName: ceph-filesystem
---
# Deployment: Training Pipeline
apiVersion: apps/v1
kind: Deployment
metadata:
  name: babylon-rl-training
  namespace: babylon-rl
  labels:
    app: babylon-rl
    component: training
spec:
  replicas: 1  # Single training job
  strategy:
    type: Recreate  # Don't run multiple trainers
  selector:
    matchLabels:
      app: babylon-rl
      component: training
  template:
    metadata:
      labels:
        app: babylon-rl
        component: training
    spec:
      containers:
      - name: training-pipeline
        image: babylonrl/training-pipeline:latest
        imagePullPolicy: Always
        command:
          - python3
          - /app/scripts/run_continuous_training.py
        env:
          - name: MODE
            value: "continuous"
          - name: TRAIN_RL_LOCAL
            value: "true"
        envFrom:
          - configMapRef:
              name: babylon-rl-config
          - secretRef:
              name: babylon-rl-secrets
        resources:
          requests:
            cpu: "8"
            memory: "64Gi"
            nvidia.com/gpu: "4"
          limits:
            cpu: "16"
            memory: "128Gi"
            nvidia.com/gpu: "4"
        volumeMounts:
          - name: checkpoints
            mountPath: /app/checkpoints
          - name: logs
            mountPath: /app/logs
        livenessProbe:
          exec:
            command:
              - pgrep
              - -f
              - run_continuous_training.py
          initialDelaySeconds: 60
          periodSeconds: 60
        readinessProbe:
          exec:
            command:
              - test
              - -f
              - /app/logs/training_pipeline.log
          initialDelaySeconds: 30
          periodSeconds: 30
      volumes:
        - name: checkpoints
          persistentVolumeClaim:
            claimName: babylon-rl-checkpoints
        - name: logs
          persistentVolumeClaim:
            claimName: babylon-rl-logs
      nodeSelector:
        gpu.nvidia.com/class: A100
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
---
# Deployment: Inference Serving (vLLM)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: babylon-rl-inference
  namespace: babylon-rl
  labels:
    app: babylon-rl
    component: inference
spec:
  replicas: 2  # For redundancy
  selector:
    matchLabels:
      app: babylon-rl
      component: inference
  template:
    metadata:
      labels:
        app: babylon-rl
        component: inference
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        args:
          - --model
          - /models/current
          - --served-model-name
          - babylon-rl
          - --tensor-parallel-size
          - "4"
          - --max-model-len
          - "4096"
          - --trust-remote-code
        ports:
          - containerPort: 8000
            name: http
        resources:
          requests:
            cpu: "8"
            memory: "64Gi"
            nvidia.com/gpu: "4"
          limits:
            cpu: "16"
            memory: "128Gi"
            nvidia.com/gpu: "4"
        volumeMounts:
          - name: checkpoints
            mountPath: /models
            readOnly: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
      volumes:
        - name: checkpoints
          persistentVolumeClaim:
            claimName: babylon-rl-checkpoints
      nodeSelector:
        gpu.nvidia.com/class: A100
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
---
# Service: Inference API
apiVersion: v1
kind: Service
metadata:
  name: babylon-rl-inference
  namespace: babylon-rl
  labels:
    app: babylon-rl
    component: inference
spec:
  type: LoadBalancer
  selector:
    app: babylon-rl
    component: inference
  ports:
    - name: http
      port: 80
      targetPort: 8000
      protocol: TCP
---
# Ingress: External access
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: babylon-rl-ingress
  namespace: babylon-rl
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - babylon-rl.coreweave.cloud
      secretName: babylon-rl-tls
  rules:
    - host: babylon-rl.coreweave.cloud
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: babylon-rl-inference
                port:
                  number: 80
---
# HorizontalPodAutoscaler: Auto-scaling inference
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: babylon-rl-inference-hpa
  namespace: babylon-rl
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: babylon-rl-inference
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# CronJob: Cleanup old checkpoints
apiVersion: batch/v1
kind: CronJob
metadata:
  name: babylon-rl-cleanup
  namespace: babylon-rl
spec:
  schedule: "0 3 * * *"  # Daily at 3am
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: cleanup
              image: babylonrl/training-pipeline:latest
              command:
                - /bin/bash
                - -c
                - |
                  echo "Cleaning up old checkpoints..."
                  find /app/checkpoints -type d -mtime +7 -exec rm -rf {} +
                  echo "Cleaning up old logs..."
                  find /app/logs -name "*.log" -mtime +7 -delete
                  echo "Cleanup complete"
              volumeMounts:
                - name: checkpoints
                  mountPath: /app/checkpoints
                - name: logs
                  mountPath: /app/logs
          volumes:
            - name: checkpoints
              persistentVolumeClaim:
                claimName: babylon-rl-checkpoints
            - name: logs
              persistentVolumeClaim:
                claimName: babylon-rl-logs
          restartPolicy: OnFailure
---
# ServiceMonitor: Prometheus monitoring
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: babylon-rl-metrics
  namespace: babylon-rl
spec:
  selector:
    matchLabels:
      app: babylon-rl
  endpoints:
    - port: metrics
      interval: 30s



