# ‚úÖ Benchmark System Integration Complete!

The benchmark system is now fully integrated with all agent types. You can run standardized performance tests across different agent implementations.

---

## üéØ What's Been Built

### 1. Core Benchmark System ‚úÖ
- **BenchmarkDataGenerator**: Creates realistic 30min+ game scenarios
- **SimulationEngine**: Replays benchmarks with fast-forward mode
- **SimulationA2AInterface**: Drop-in A2A replacement for simulation
- **MetricsVisualizer**: Generates HTML reports and visualizations
- **Comprehensive Tests**: All systems validated and passing

### 2. Eliza Agent Integration ‚úÖ
**File**: `scripts/run-eliza-benchmark.ts`

Runs Eliza agents (integrated into Babylon) through benchmarks:
- Uses real AutonomousCoordinator
- Full database persistence
- Trajectory recording for RL training
- Comprehensive metrics

### 3. Autonomous Babylon Agent (TypeScript) ‚úÖ
**File**: `examples/autonomous-babylon-agent/src/benchmark-runner.ts`

Standalone TypeScript agent benchmark runner:
- LLM-powered decision making
- Configurable strategies
- Memory management
- Direct A2A integration

### 4. Babylon LangGraph Agent (Python) ‚úÖ
**File**: `examples/babylon-langgraph-agent/benchmark_runner.py`

Python agent benchmark runner:
- LangGraph state management
- Async A2A interface
- Extensible for custom agents
- Python-native metrics

### 5. Complete Documentation ‚úÖ
- **BENCHMARK_SYSTEM.md**: Core system guide
- **BENCHMARK_COMPARISON_GUIDE.md**: Multi-agent comparison
- **BENCHMARK_README.md**: Per-agent instructions
- **Code Examples**: Full working implementations

---

## üöÄ Quick Start Guide

### Step 1: Generate a Benchmark

```bash
cd /Users/shawwalters/babylon

npx ts-node scripts/generate-benchmark.ts \
  --duration=30 \
  --interval=60 \
  --markets=8 \
  --perpetuals=5 \
  --agents=10 \
  --seed=12345
```

Output: `benchmarks/benchmark-{id}.json`

### Step 2: Run All Agent Types

```bash
# Save benchmark path
BENCHMARK="benchmarks/benchmark-{id}.json"

# 1Ô∏è‚É£ Eliza Agent
npx ts-node scripts/run-eliza-benchmark.ts \
  --agent-id=your-eliza-agent-id \
  --benchmark=$BENCHMARK \
  --runs=5 \
  --output=results/eliza

# 2Ô∏è‚É£ Autonomous Babylon Agent (TypeScript)
cd examples/autonomous-babylon-agent
bun run benchmark \
  --benchmark=../../$BENCHMARK \
  --runs=5 \
  --output=benchmark-results/autonomous
cd ../..

# 3Ô∏è‚É£ LangGraph Agent (Python)
cd examples/babylon-langgraph-agent
source .venv/bin/activate
python benchmark_runner.py \
  --benchmark=../../$BENCHMARK \
  --runs=5 \
  --output=benchmark-results/langgraph
cd ../..
```

### Step 3: Compare Results

```bash
# View HTML reports
open results/eliza/comparison.html
open examples/autonomous-babylon-agent/benchmark-results/autonomous/index.html

# Compare JSON
echo "Eliza:"
jq '.comparison' results/eliza/comparison.json

echo "Autonomous:"
jq '.comparison' examples/autonomous-babylon-agent/benchmark-results/autonomous/comparison.json

echo "LangGraph:"
jq '.comparison' examples/babylon-langgraph-agent/benchmark-results/langgraph/comparison.json
```

---

## üìä Comparison Table

All agents run on identical benchmarks, producing comparable metrics:

| Metric | Description | All Agents? |
|--------|-------------|-------------|
| **Total P&L** | Net profit/loss | ‚úÖ |
| **Prediction Accuracy** | % correct predictions | ‚úÖ |
| **Perp Win Rate** | % profitable perp trades | ‚úÖ |
| **Optimality Score** | How close to perfect play (0-100%) | ‚úÖ |
| **Avg Response Time** | Decision latency | ‚úÖ |
| **Actions Breakdown** | Trade/post/social counts | ‚úÖ |
| **Social Engagement** | Posts, groups, reputation | ‚úÖ |

---

## üéÆ Agent-Specific Features

### Eliza Agent
```bash
npx ts-node scripts/run-eliza-benchmark.ts \
  --agent-id=cm2abc123 \
  --benchmark=benchmarks/test.json \
  --runs=5 \
  --tick-interval=5000
```

**Features**:
- Real autonomous coordinator
- Database persistence
- Trajectory recording for RL
- Memory and context management

**Best For**:
- Production agent testing
- RL training data generation
- End-to-end validation

### Autonomous Babylon Agent (TypeScript)
```bash
cd examples/autonomous-babylon-agent
bun run benchmark \
  --benchmark=../../benchmarks/test.json \
  --runs=5
```

**Features**:
- Strategy testing (aggressive/conservative/balanced)
- LLM provider comparison
- Lightweight and fast
- Easy to modify

**Best For**:
- Strategy development
- Quick iteration
- LLM provider testing

### LangGraph Agent (Python)
```bash
cd examples/babylon-langgraph-agent
python benchmark_runner.py \
  --benchmark=../../benchmarks/test.json \
  --runs=5
```

**Features**:
- LangGraph integration
- Python ecosystem
- Complex state management
- Custom agent logic

**Best For**:
- LangGraph development
- Python-based agents
- Complex decision flows

---

## üìÅ File Structure

```
babylon/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ generate-benchmark.ts           # Generate benchmarks
‚îÇ   ‚îî‚îÄ‚îÄ run-eliza-benchmark.ts          # Run Eliza agents
‚îÇ
‚îú‚îÄ‚îÄ src/lib/benchmark/
‚îÇ   ‚îú‚îÄ‚îÄ BenchmarkDataGenerator.ts       # Generate game data
‚îÇ   ‚îú‚îÄ‚îÄ SimulationEngine.ts             # Replay simulation
‚îÇ   ‚îú‚îÄ‚îÄ SimulationA2AInterface.ts       # A2A mock
‚îÇ   ‚îú‚îÄ‚îÄ BenchmarkRunner.ts              # Orchestration
‚îÇ   ‚îî‚îÄ‚îÄ MetricsVisualizer.ts            # HTML reports
‚îÇ
‚îú‚îÄ‚îÄ examples/autonomous-babylon-agent/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ benchmark-runner.ts         # TypeScript agent runner
‚îÇ   ‚îî‚îÄ‚îÄ BENCHMARK_README.md             # Instructions
‚îÇ
‚îú‚îÄ‚îÄ examples/babylon-langgraph-agent/
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_runner.py             # Python agent runner
‚îÇ   ‚îî‚îÄ‚îÄ BENCHMARK_README.md             # Instructions
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ BENCHMARK_SYSTEM.md             # Core system docs
‚îÇ   ‚îî‚îÄ‚îÄ BENCHMARK_COMPARISON_GUIDE.md   # Comparison guide
‚îÇ
‚îú‚îÄ‚îÄ benchmarks/                          # Generated benchmarks
‚îÇ   ‚îî‚îÄ‚îÄ benchmark-*.json
‚îÇ
‚îî‚îÄ‚îÄ results/                             # Benchmark results
    ‚îú‚îÄ‚îÄ eliza/
    ‚îú‚îÄ‚îÄ autonomous/
    ‚îî‚îÄ‚îÄ langgraph/
```

---

## üî• Example Workflows

### Workflow 1: Compare Agent Types

```bash
#!/bin/bash
# compare-agents.sh

BENCHMARK="benchmarks/standard.json"
RUNS=10

# Generate benchmark once
npx ts-node scripts/generate-benchmark.ts \
  --duration=30 \
  --seed=42 \
  > $BENCHMARK

# Run all agents
echo "Running Eliza..."
npx ts-node scripts/run-eliza-benchmark.ts \
  --agent-id=cm2abc123 \
  --benchmark=$BENCHMARK \
  --runs=$RUNS \
  --output=results/eliza

echo "Running Autonomous..."
cd examples/autonomous-babylon-agent
bun run benchmark \
  --benchmark=../../$BENCHMARK \
  --runs=$RUNS \
  --output=benchmark-results/test
cd ../..

echo "Running LangGraph..."
cd examples/babylon-langgraph-agent
python benchmark_runner.py \
  --benchmark=../../$BENCHMARK \
  --runs=$RUNS \
  --output=benchmark-results/test
cd ../..

# Compare
echo ""
echo "=== RESULTS ==="
jq '{type: "eliza", pnl: .comparison.avgPnl, accuracy: .comparison.avgAccuracy}' results/eliza/comparison.json
jq '{type: "autonomous", pnl: .comparison.avgPnl, accuracy: .comparison.avgAccuracy}' examples/autonomous-babylon-agent/benchmark-results/test/comparison.json
jq '{type: "langgraph", pnl: .comparison.avgPnl, accuracy: .comparison.avgAccuracy}' examples/babylon-langgraph-agent/benchmark-results/test/comparison.json
```

### Workflow 2: Test Strategy Changes

```bash
#!/bin/bash
# test-strategies.sh

BENCHMARK="benchmarks/test.json"

cd examples/autonomous-babylon-agent

# Test each strategy
for strategy in aggressive conservative balanced; do
  echo "Testing $strategy strategy..."
  AGENT_STRATEGY=$strategy bun run benchmark \
    --benchmark=../../$BENCHMARK \
    --runs=5 \
    --output=results/$strategy
done

# Compare
echo ""
echo "=== STRATEGY COMPARISON ==="
for strategy in aggressive conservative balanced; do
  echo "$strategy:"
  jq '.comparison.avgPnl' results/$strategy/comparison.json
done
```

### Workflow 3: Continuous Monitoring

```bash
#!/bin/bash
# daily-benchmark.sh

DATE=$(date +%Y%m%d)
BENCHMARK="benchmarks/daily-$DATE.json"

# Generate fresh benchmark
npx ts-node scripts/generate-benchmark.ts \
  --duration=30 \
  --seed=$DATE \
  > $BENCHMARK

# Run all agents
npx ts-node scripts/run-eliza-benchmark.ts \
  --agent-id=production-agent \
  --benchmark=$BENCHMARK \
  --runs=5 \
  --output=monitoring/$DATE/eliza

# Save results
git add monitoring/$DATE/
git commit -m "Daily benchmark: $DATE"
```

---

## üéØ Use Cases

### 1. Pre-Deployment Validation
Test new models before deploying:
```bash
# Test new model
npx ts-node scripts/run-eliza-benchmark.ts \
  --agent-id=new-model-agent \
  --benchmark=benchmarks/standard.json \
  --runs=10

# Compare with baseline
diff results/baseline/metrics.json results/new-model/metrics.json
```

### 2. A/B Testing
Compare two strategies:
```bash
# Strategy A
AGENT_STRATEGY=aggressive bun run benchmark ...

# Strategy B  
AGENT_STRATEGY=conservative bun run benchmark ...

# Compare P&L
```

### 3. Regression Testing
Ensure changes don't hurt performance:
```bash
# Run suite of benchmarks
for bench in benchmarks/*.json; do
  npx ts-node scripts/run-eliza-benchmark.ts \
    --agent-id=my-agent \
    --benchmark=$bench
done
```

### 4. Generate Training Data
Create GRPO training datasets:
```bash
# Generate 10 different scenarios
for i in {1..10}; do
  npx ts-node scripts/generate-benchmark.ts --seed=$i
done

# Run agent through all (saves trajectories)
for bench in benchmarks/*.json; do
  npx ts-node scripts/run-eliza-benchmark.ts \
    --agent-id=my-agent \
    --benchmark=$bench \
    --runs=5
done
```

---

## ‚úÖ Verification Checklist

Before using the system, verify:

### Core System
- [ ] Build complete: `npm run build`
- [ ] Tests passing: `npm test tests/benchmark-system.test.ts`
- [ ] Can generate benchmark: `npx ts-node scripts/generate-benchmark.ts`

### Eliza Integration
- [ ] Agent exists in database
- [ ] Agent has `isAgent = true`
- [ ] Can run benchmark: `npx ts-node scripts/run-eliza-benchmark.ts --agent-id=xxx --duration=5`

### Autonomous Agent
- [ ] Dependencies installed: `cd examples/autonomous-babylon-agent && bun install`
- [ ] Environment configured: `.env.local` has API keys
- [ ] Can run benchmark: `bun run benchmark --benchmark=../../benchmarks/test.json`

### LangGraph Agent
- [ ] Virtual env setup: `python -m venv .venv`
- [ ] Dependencies installed: `pip install -e .`
- [ ] Can run benchmark: `python benchmark_runner.py --benchmark=../../benchmarks/test.json`

---

## üÜò Troubleshooting

### "Cannot find benchmark modules"
```bash
# Build main project
npm run build

# Verify dist/ exists
ls dist/src/lib/benchmark/
```

### "Agent not found"
```bash
# Check database
psql $DATABASE_URL -c "SELECT id, username, \"isAgent\" FROM \"User\" WHERE id = 'your-agent-id';"

# Verify isAgent is true
```

### "Import error"
```bash
# TypeScript agent
cd examples/autonomous-babylon-agent
bun install

# Python agent
cd examples/babylon-langgraph-agent
source .venv/bin/activate
pip install -e .
```

### "LLM provider not available"
```bash
# Check environment variables
cat .env.local | grep API_KEY

# Test API key
curl https://api.groq.com/openai/v1/models \
  -H "Authorization: Bearer $GROQ_API_KEY"
```

---

## üìö Documentation

Complete documentation available:

1. **[BENCHMARK_SYSTEM.md](docs/BENCHMARK_SYSTEM.md)**
   - Core system architecture
   - Data format specification
   - API reference

2. **[BENCHMARK_COMPARISON_GUIDE.md](docs/BENCHMARK_COMPARISON_GUIDE.md)**
   - Multi-agent comparison
   - Best practices
   - Advanced usage

3. **Agent-Specific READMEs**
   - [Autonomous Agent](examples/autonomous-babylon-agent/BENCHMARK_README.md)
   - [LangGraph Agent](examples/babylon-langgraph-agent/BENCHMARK_README.md)

4. **[RL_TRAINING_COMPLETE_GUIDE.md](RL_TRAINING_COMPLETE_GUIDE.md)**
   - RL training integration
   - Trajectory recording
   - GRPO training

---

## üéâ You're Ready!

The complete benchmark system is now integrated and ready to use. You can:

‚úÖ Generate standardized game scenarios  
‚úÖ Run Eliza agents through benchmarks  
‚úÖ Run TypeScript example agents  
‚úÖ Run Python example agents  
‚úÖ Compare results across all types  
‚úÖ Track performance over time  
‚úÖ Generate RL training data  

**Next Steps**:
1. Generate your first benchmark
2. Run all three agent types
3. Compare the results
4. Iterate and improve!

---

**Built with ‚ù§Ô∏è for Babylon RL Training**

*Last Updated: November 13, 2025*




The benchmark system is now fully integrated with all agent types. You can run standardized performance tests across different agent implementations.

---

## üéØ What's Been Built

### 1. Core Benchmark System ‚úÖ
- **BenchmarkDataGenerator**: Creates realistic 30min+ game scenarios
- **SimulationEngine**: Replays benchmarks with fast-forward mode
- **SimulationA2AInterface**: Drop-in A2A replacement for simulation
- **MetricsVisualizer**: Generates HTML reports and visualizations
- **Comprehensive Tests**: All systems validated and passing

### 2. Eliza Agent Integration ‚úÖ
**File**: `scripts/run-eliza-benchmark.ts`

Runs Eliza agents (integrated into Babylon) through benchmarks:
- Uses real AutonomousCoordinator
- Full database persistence
- Trajectory recording for RL training
- Comprehensive metrics

### 3. Autonomous Babylon Agent (TypeScript) ‚úÖ
**File**: `examples/autonomous-babylon-agent/src/benchmark-runner.ts`

Standalone TypeScript agent benchmark runner:
- LLM-powered decision making
- Configurable strategies
- Memory management
- Direct A2A integration

### 4. Babylon LangGraph Agent (Python) ‚úÖ
**File**: `examples/babylon-langgraph-agent/benchmark_runner.py`

Python agent benchmark runner:
- LangGraph state management
- Async A2A interface
- Extensible for custom agents
- Python-native metrics

### 5. Complete Documentation ‚úÖ
- **BENCHMARK_SYSTEM.md**: Core system guide
- **BENCHMARK_COMPARISON_GUIDE.md**: Multi-agent comparison
- **BENCHMARK_README.md**: Per-agent instructions
- **Code Examples**: Full working implementations

---

## üöÄ Quick Start Guide

### Step 1: Generate a Benchmark

```bash
cd /Users/shawwalters/babylon

npx ts-node scripts/generate-benchmark.ts \
  --duration=30 \
  --interval=60 \
  --markets=8 \
  --perpetuals=5 \
  --agents=10 \
  --seed=12345
```

Output: `benchmarks/benchmark-{id}.json`

### Step 2: Run All Agent Types

```bash
# Save benchmark path
BENCHMARK="benchmarks/benchmark-{id}.json"

# 1Ô∏è‚É£ Eliza Agent
npx ts-node scripts/run-eliza-benchmark.ts \
  --agent-id=your-eliza-agent-id \
  --benchmark=$BENCHMARK \
  --runs=5 \
  --output=results/eliza

# 2Ô∏è‚É£ Autonomous Babylon Agent (TypeScript)
cd examples/autonomous-babylon-agent
bun run benchmark \
  --benchmark=../../$BENCHMARK \
  --runs=5 \
  --output=benchmark-results/autonomous
cd ../..

# 3Ô∏è‚É£ LangGraph Agent (Python)
cd examples/babylon-langgraph-agent
source .venv/bin/activate
python benchmark_runner.py \
  --benchmark=../../$BENCHMARK \
  --runs=5 \
  --output=benchmark-results/langgraph
cd ../..
```

### Step 3: Compare Results

```bash
# View HTML reports
open results/eliza/comparison.html
open examples/autonomous-babylon-agent/benchmark-results/autonomous/index.html

# Compare JSON
echo "Eliza:"
jq '.comparison' results/eliza/comparison.json

echo "Autonomous:"
jq '.comparison' examples/autonomous-babylon-agent/benchmark-results/autonomous/comparison.json

echo "LangGraph:"
jq '.comparison' examples/babylon-langgraph-agent/benchmark-results/langgraph/comparison.json
```

---

## üìä Comparison Table

All agents run on identical benchmarks, producing comparable metrics:

| Metric | Description | All Agents? |
|--------|-------------|-------------|
| **Total P&L** | Net profit/loss | ‚úÖ |
| **Prediction Accuracy** | % correct predictions | ‚úÖ |
| **Perp Win Rate** | % profitable perp trades | ‚úÖ |
| **Optimality Score** | How close to perfect play (0-100%) | ‚úÖ |
| **Avg Response Time** | Decision latency | ‚úÖ |
| **Actions Breakdown** | Trade/post/social counts | ‚úÖ |
| **Social Engagement** | Posts, groups, reputation | ‚úÖ |

---

## üéÆ Agent-Specific Features

### Eliza Agent
```bash
npx ts-node scripts/run-eliza-benchmark.ts \
  --agent-id=cm2abc123 \
  --benchmark=benchmarks/test.json \
  --runs=5 \
  --tick-interval=5000
```

**Features**:
- Real autonomous coordinator
- Database persistence
- Trajectory recording for RL
- Memory and context management

**Best For**:
- Production agent testing
- RL training data generation
- End-to-end validation

### Autonomous Babylon Agent (TypeScript)
```bash
cd examples/autonomous-babylon-agent
bun run benchmark \
  --benchmark=../../benchmarks/test.json \
  --runs=5
```

**Features**:
- Strategy testing (aggressive/conservative/balanced)
- LLM provider comparison
- Lightweight and fast
- Easy to modify

**Best For**:
- Strategy development
- Quick iteration
- LLM provider testing

### LangGraph Agent (Python)
```bash
cd examples/babylon-langgraph-agent
python benchmark_runner.py \
  --benchmark=../../benchmarks/test.json \
  --runs=5
```

**Features**:
- LangGraph integration
- Python ecosystem
- Complex state management
- Custom agent logic

**Best For**:
- LangGraph development
- Python-based agents
- Complex decision flows

---

## üìÅ File Structure

```
babylon/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ generate-benchmark.ts           # Generate benchmarks
‚îÇ   ‚îî‚îÄ‚îÄ run-eliza-benchmark.ts          # Run Eliza agents
‚îÇ
‚îú‚îÄ‚îÄ src/lib/benchmark/
‚îÇ   ‚îú‚îÄ‚îÄ BenchmarkDataGenerator.ts       # Generate game data
‚îÇ   ‚îú‚îÄ‚îÄ SimulationEngine.ts             # Replay simulation
‚îÇ   ‚îú‚îÄ‚îÄ SimulationA2AInterface.ts       # A2A mock
‚îÇ   ‚îú‚îÄ‚îÄ BenchmarkRunner.ts              # Orchestration
‚îÇ   ‚îî‚îÄ‚îÄ MetricsVisualizer.ts            # HTML reports
‚îÇ
‚îú‚îÄ‚îÄ examples/autonomous-babylon-agent/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ benchmark-runner.ts         # TypeScript agent runner
‚îÇ   ‚îî‚îÄ‚îÄ BENCHMARK_README.md             # Instructions
‚îÇ
‚îú‚îÄ‚îÄ examples/babylon-langgraph-agent/
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_runner.py             # Python agent runner
‚îÇ   ‚îî‚îÄ‚îÄ BENCHMARK_README.md             # Instructions
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ BENCHMARK_SYSTEM.md             # Core system docs
‚îÇ   ‚îî‚îÄ‚îÄ BENCHMARK_COMPARISON_GUIDE.md   # Comparison guide
‚îÇ
‚îú‚îÄ‚îÄ benchmarks/                          # Generated benchmarks
‚îÇ   ‚îî‚îÄ‚îÄ benchmark-*.json
‚îÇ
‚îî‚îÄ‚îÄ results/                             # Benchmark results
    ‚îú‚îÄ‚îÄ eliza/
    ‚îú‚îÄ‚îÄ autonomous/
    ‚îî‚îÄ‚îÄ langgraph/
```

---

## üî• Example Workflows

### Workflow 1: Compare Agent Types

```bash
#!/bin/bash
# compare-agents.sh

BENCHMARK="benchmarks/standard.json"
RUNS=10

# Generate benchmark once
npx ts-node scripts/generate-benchmark.ts \
  --duration=30 \
  --seed=42 \
  > $BENCHMARK

# Run all agents
echo "Running Eliza..."
npx ts-node scripts/run-eliza-benchmark.ts \
  --agent-id=cm2abc123 \
  --benchmark=$BENCHMARK \
  --runs=$RUNS \
  --output=results/eliza

echo "Running Autonomous..."
cd examples/autonomous-babylon-agent
bun run benchmark \
  --benchmark=../../$BENCHMARK \
  --runs=$RUNS \
  --output=benchmark-results/test
cd ../..

echo "Running LangGraph..."
cd examples/babylon-langgraph-agent
python benchmark_runner.py \
  --benchmark=../../$BENCHMARK \
  --runs=$RUNS \
  --output=benchmark-results/test
cd ../..

# Compare
echo ""
echo "=== RESULTS ==="
jq '{type: "eliza", pnl: .comparison.avgPnl, accuracy: .comparison.avgAccuracy}' results/eliza/comparison.json
jq '{type: "autonomous", pnl: .comparison.avgPnl, accuracy: .comparison.avgAccuracy}' examples/autonomous-babylon-agent/benchmark-results/test/comparison.json
jq '{type: "langgraph", pnl: .comparison.avgPnl, accuracy: .comparison.avgAccuracy}' examples/babylon-langgraph-agent/benchmark-results/test/comparison.json
```

### Workflow 2: Test Strategy Changes

```bash
#!/bin/bash
# test-strategies.sh

BENCHMARK="benchmarks/test.json"

cd examples/autonomous-babylon-agent

# Test each strategy
for strategy in aggressive conservative balanced; do
  echo "Testing $strategy strategy..."
  AGENT_STRATEGY=$strategy bun run benchmark \
    --benchmark=../../$BENCHMARK \
    --runs=5 \
    --output=results/$strategy
done

# Compare
echo ""
echo "=== STRATEGY COMPARISON ==="
for strategy in aggressive conservative balanced; do
  echo "$strategy:"
  jq '.comparison.avgPnl' results/$strategy/comparison.json
done
```

### Workflow 3: Continuous Monitoring

```bash
#!/bin/bash
# daily-benchmark.sh

DATE=$(date +%Y%m%d)
BENCHMARK="benchmarks/daily-$DATE.json"

# Generate fresh benchmark
npx ts-node scripts/generate-benchmark.ts \
  --duration=30 \
  --seed=$DATE \
  > $BENCHMARK

# Run all agents
npx ts-node scripts/run-eliza-benchmark.ts \
  --agent-id=production-agent \
  --benchmark=$BENCHMARK \
  --runs=5 \
  --output=monitoring/$DATE/eliza

# Save results
git add monitoring/$DATE/
git commit -m "Daily benchmark: $DATE"
```

---

## üéØ Use Cases

### 1. Pre-Deployment Validation
Test new models before deploying:
```bash
# Test new model
npx ts-node scripts/run-eliza-benchmark.ts \
  --agent-id=new-model-agent \
  --benchmark=benchmarks/standard.json \
  --runs=10

# Compare with baseline
diff results/baseline/metrics.json results/new-model/metrics.json
```

### 2. A/B Testing
Compare two strategies:
```bash
# Strategy A
AGENT_STRATEGY=aggressive bun run benchmark ...

# Strategy B  
AGENT_STRATEGY=conservative bun run benchmark ...

# Compare P&L
```

### 3. Regression Testing
Ensure changes don't hurt performance:
```bash
# Run suite of benchmarks
for bench in benchmarks/*.json; do
  npx ts-node scripts/run-eliza-benchmark.ts \
    --agent-id=my-agent \
    --benchmark=$bench
done
```

### 4. Generate Training Data
Create GRPO training datasets:
```bash
# Generate 10 different scenarios
for i in {1..10}; do
  npx ts-node scripts/generate-benchmark.ts --seed=$i
done

# Run agent through all (saves trajectories)
for bench in benchmarks/*.json; do
  npx ts-node scripts/run-eliza-benchmark.ts \
    --agent-id=my-agent \
    --benchmark=$bench \
    --runs=5
done
```

---

## ‚úÖ Verification Checklist

Before using the system, verify:

### Core System
- [ ] Build complete: `npm run build`
- [ ] Tests passing: `npm test tests/benchmark-system.test.ts`
- [ ] Can generate benchmark: `npx ts-node scripts/generate-benchmark.ts`

### Eliza Integration
- [ ] Agent exists in database
- [ ] Agent has `isAgent = true`
- [ ] Can run benchmark: `npx ts-node scripts/run-eliza-benchmark.ts --agent-id=xxx --duration=5`

### Autonomous Agent
- [ ] Dependencies installed: `cd examples/autonomous-babylon-agent && bun install`
- [ ] Environment configured: `.env.local` has API keys
- [ ] Can run benchmark: `bun run benchmark --benchmark=../../benchmarks/test.json`

### LangGraph Agent
- [ ] Virtual env setup: `python -m venv .venv`
- [ ] Dependencies installed: `pip install -e .`
- [ ] Can run benchmark: `python benchmark_runner.py --benchmark=../../benchmarks/test.json`

---

## üÜò Troubleshooting

### "Cannot find benchmark modules"
```bash
# Build main project
npm run build

# Verify dist/ exists
ls dist/src/lib/benchmark/
```

### "Agent not found"
```bash
# Check database
psql $DATABASE_URL -c "SELECT id, username, \"isAgent\" FROM \"User\" WHERE id = 'your-agent-id';"

# Verify isAgent is true
```

### "Import error"
```bash
# TypeScript agent
cd examples/autonomous-babylon-agent
bun install

# Python agent
cd examples/babylon-langgraph-agent
source .venv/bin/activate
pip install -e .
```

### "LLM provider not available"
```bash
# Check environment variables
cat .env.local | grep API_KEY

# Test API key
curl https://api.groq.com/openai/v1/models \
  -H "Authorization: Bearer $GROQ_API_KEY"
```

---

## üìö Documentation

Complete documentation available:

1. **[BENCHMARK_SYSTEM.md](docs/BENCHMARK_SYSTEM.md)**
   - Core system architecture
   - Data format specification
   - API reference

2. **[BENCHMARK_COMPARISON_GUIDE.md](docs/BENCHMARK_COMPARISON_GUIDE.md)**
   - Multi-agent comparison
   - Best practices
   - Advanced usage

3. **Agent-Specific READMEs**
   - [Autonomous Agent](examples/autonomous-babylon-agent/BENCHMARK_README.md)
   - [LangGraph Agent](examples/babylon-langgraph-agent/BENCHMARK_README.md)

4. **[RL_TRAINING_COMPLETE_GUIDE.md](RL_TRAINING_COMPLETE_GUIDE.md)**
   - RL training integration
   - Trajectory recording
   - GRPO training

---

## üéâ You're Ready!

The complete benchmark system is now integrated and ready to use. You can:

‚úÖ Generate standardized game scenarios  
‚úÖ Run Eliza agents through benchmarks  
‚úÖ Run TypeScript example agents  
‚úÖ Run Python example agents  
‚úÖ Compare results across all types  
‚úÖ Track performance over time  
‚úÖ Generate RL training data  

**Next Steps**:
1. Generate your first benchmark
2. Run all three agent types
3. Compare the results
4. Iterate and improve!

---

**Built with ‚ù§Ô∏è for Babylon RL Training**

*Last Updated: November 13, 2025*


